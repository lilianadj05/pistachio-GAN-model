# ğŸ¥œ Pistachio GAN Model

## ğŸ“Œ Project Overview
This project uses a Generative Adversarial Network (GAN) to generate new images based on a provided dataset. The goal is to train a model that can create realistic-looking images that are indistinguishable from the real images in the dataset. The model's performance is evaluated using the FrÃ©chet Inception Distance (FID) score.

## ğŸ“Š Dataset
The pistachio images in this dataset are all a uniform size of 600x600 pixels and have a square aspect ratio. An analysis of the pixel brightness distribution shows a balanced variation in lighting, with a symmetrical distribution peaking around a medium intensity. The images were preprocessed by resizing and normalizing the pixel values to a range of [-1, 1].

## ğŸ” Key Findings
1. The tuned model showed better performance than the baseline model, with a better FID score.
2. The images generated by the tuned model were visually more similar to the actual images in the dataset.
3. The overall performance of both models was not optimal. This could be due to a simple model architecture, a small dataset, or insufficient training epochs.

## ğŸ“ˆ Steps
1. **Data Cleaning & Preprocessing**: The images were loaded and inspected for consistency. The images were resized to 100x100 pixels and normalized to a range of [-1, 1].
2. **Modelling**: A GAN model was built, consisting of a generator and a discriminator. The generator and discriminator models were trained using the Adam optimizer and binary cross-entropy as the loss function.
3. **Evaluation**: The model's performance was evaluated by generating images after every 10 epochs and comparing the FID score of the tuned model with the baseline model.

## ğŸ’¡ Conclusion
While the tuned model performed better than the baseline, the overall results were not optimal. The FID score of the tuned model was lower, and the images it produced were more realistic. However, the models might need further refinement.

## ğŸ”® Possible Future Works
1.**More Complex Model Architecture**: Upgrade the current model to more advanced architectures like a Deep Convolutional GAN (DCGAN) or Wasserworth GAN (WGAN) to improve training stability and image quality.
2. **Larger Dataset**: Using a larger dataset would provide the model with more diverse examples to learn from. This helps prevent overfitting and allows the generator to create higher-quality images that aren't just memorizing the limited training data.
3. **More Training Epochs**: Increasing the number of epochs would allow the model to train for a longer duration, helping it converge to a more optimal state.
4.**Hyperparameter Tuning**: Fine-tuning the hyperparameters, such as learning rates and batch sizes, could further optimize the model's performance. Systematic approaches like Grid Search or Random Search can be used to find the best combination of values that maximizes the model's effectiveness.

## ğŸ‘¨â€ğŸ’» Author
**Liliana Djaja Witama** | Undergraduate Data Science Student at BINUS University
